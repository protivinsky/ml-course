{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n",
    "\n",
    "![Reinforcement Learning](img/reinforcement-learning.jpg)\n",
    "\n",
    "1. Being greedy doesn't always work\n",
    "2. Sequence matters in Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Reinforcement Learning Process\n",
    "\n",
    "1. Observation of the environment\n",
    "2. Deciding how to act using some strategy\n",
    "3. Acting accordingly\n",
    "4. Receiving a reward or penalty\n",
    "5. Learning from the experiences and refining our strategy\n",
    "6. Iterate until an optimal strategy is found\n",
    "\n",
    "![RL Animation](img/RL-animation.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Design: Self-Driving Cab\n",
    "\n",
    "The Smartcab's job is to pick up the passenger at one location and drop them off in another. Here are a few things that we'd love our Smartcab to take care of:\n",
    "\n",
    "Drop off the passenger to the right location.\n",
    "Save passenger's time by taking minimum time possible to drop off\n",
    "Take care of passenger's safety and traffic rules\n",
    "There are different aspects that need to be considered here while modeling an RL solution to this problem: rewards, states, and actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Rewards\n",
    "Since the agent (the imaginary driver) is reward-motivated and is going to learn how to control the cab by trial experiences in the environment, we need to decide the **rewards and/or penalties** and their magnitude accordingly. Here a few points to consider:\n",
    "\n",
    "- a high positive reward for a successful dropoff because this behavior is highly desired\n",
    "- penalty if the driver tries to drop off a passenger in wrong locations\n",
    "- a slight negative reward for not making it to the destination after every time-step. \"Slight\" negative because we would prefer our agent to reach late instead of making wrong moves trying to reach to the destination as fast as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. State Space\n",
    "In Reinforcement Learning, the agent encounters a state, and then takes action according to the state it's in.\n",
    "\n",
    "The **state space** is the set of all possible situations our taxi could inhabit. The state should contain useful information the agent needs to make the right action.\n",
    "\n",
    "Let's say we have a training area for our Smartcab where we are teaching it to transport people in a parking lot to four different locations (R, G, Y, B):\n",
    "\n",
    "![Open AI Taxi](img/open-ai-taxi.jpg)\n",
    "\n",
    "Let's assume Smartcab is the only vehicle in this parking lot. We can break up the parking lot into a 5x5 grid, which gives us 25 possible taxi locations. These 25 locations are one part of our state space. Notice the current location state of our taxi is coordinate (3, 1).\n",
    "\n",
    "You'll also notice there are four (4) locations that we can pick up and drop off a passenger: R, G, Y, B or `[(0,0), (0,4), (4,0), (4,3)]` in (row, col) coordinates. Our illustrated passenger is in location Y and they wish to go to location R.\n",
    "\n",
    "When we also account for one (1) additional passenger state of being inside the taxi, we can take all combinations of passenger locations and destination locations to come to a total number of states for our taxi environment; there's four (4) destinations and five (4 + 1) passenger locations.\n",
    "\n",
    "So, our taxi environment has 5×5×5×4=500 total possible states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Action Space\n",
    "The agent encounters one of the 500 states and it takes an action. The action in our case can be to move in a direction or decide to pickup/dropoff a passenger. In other words, we have six possible actions:\n",
    "\n",
    "- `0 = south`\n",
    "- `1 = north`\n",
    "- `2 = east`\n",
    "- `3 = west`\n",
    "- `4 = pickup`\n",
    "- `5 = dropoff`\n",
    "\n",
    "This is the **action space**: the set of all the actions that our agent can take in a given state.\n",
    "\n",
    "You'll notice in the illustration above, that the taxi cannot perform certain actions in certain states due to walls. In environment's code, we will simply provide a -1 penalty for every wall hit and the taxi won't move anywhere. This will just rack up penalties causing the taxi to consider going around the wall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation with Python\n",
    "\n",
    "The gym environment is provided by https://gym.openai.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-deps -e src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R:\u001b[43m \u001b[0m| : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import seaborn as sns\n",
    "\n",
    "env = gym.make('Taxi-v3').env\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_\"There are 4 locations (labeled by different letters), and our job is to pick up the passenger at one location and drop him \n",
    "off at another. We receive +20 points for a successful drop-off and lose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\"_\n",
    "\n",
    "- The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\n",
    "- The pipe (\"|\") represents a wall which the taxi cannot cross.\n",
    "- R, G, Y, B are the possible pickup and destination locations. The blue letter represents the current passenger pick-up location, and the purple letter is the current destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | :\u001b[43m \u001b[0m:G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 328\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Reward Table\n",
    "\n",
    "When the Taxi environment is created, there is an initial Reward table that's also created, called `P`. We can think of it like a matrix that has the number of states as rows and number of actions as columns, i.e. a states × actions matrix.\n",
    "\n",
    "Actions are: `0 = south`, `1 = north`, `2 = east`, `3 = west`, `4 = pickup`, `5 = dropoff`\n",
    "\n",
    "Since every state is in this matrix, we can see the default reward values assigned to our illustration's state. This dictionary has the structure `{action: [(probability, nextstate, reward, done)]}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, -1, False)],\n",
       " 1: [(1.0, 228, -1, False)],\n",
       " 2: [(1.0, 348, -1, False)],\n",
       " 3: [(1.0, 328, -1, False)],\n",
       " 4: [(1.0, 328, -10, False)],\n",
       " 5: [(1.0, 328, -10, False)]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[328]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to note:\n",
    "\n",
    "- The 0-5 corresponds to the actions (south, north, east, west, pickup, dropoff) the taxi can perform at our current state in the illustration.\n",
    "- In this env, `probability` is always 1.0.\n",
    "- The `nextstate` is the state we would be in if we take the action at this index of the dict\n",
    "- All the movement actions have a -1 reward and the pickup/dropoff actions have -10 reward in this particular state. If we are in a state where the taxi has a passenger and is on top of the right destination, we would see a reward of 20 at the dropoff action (5)\n",
    "- `done` is used to tell us when we have successfully dropped off a passenger in the right location. Each successfull dropoff is the end of an episode\n",
    "\n",
    "Note that if our agent chose to explore action two (2) in this state it would be going East into a wall. The source code has made it impossible to actually move the taxi across a wall, so if the taxi chooses that action, it will just keep accruing -1 penalties, which affects the long-term reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the environment without Reinforcement Learning\n",
    "\n",
    "Let's just use a random drive - taking a random action at each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 123\n",
      "Penalties incurred: 37\n"
     ]
    }
   ],
   "source": [
    "env.s = 328  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 123\n",
      "State: 0\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(frames[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning in Python\n",
    "\n",
    "![Q-Learning](img/q-learning.jpg)\n",
    "\n",
    "- Q-table with Q-values, for each `(state, action)` pair - representative of the quality of an action in the particular state\n",
    "- alpha is the learning rate [0, 1]\n",
    "- gamma is the discount factor [0, 1]\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Initialize the Q-table by all zeros.\n",
    "2. Start exploring actions: For each state, select any one among all possible actions for the current state (S).\n",
    "3. Travel to the next state (S') as a result of that action (a).\n",
    "4. For all possible actions from the state (S') select the one with the highest Q-value.\n",
    "5. Update Q-table values using the equation.\n",
    "6. Set the next state as the current state.\n",
    "7. If goal state is reached, then end and repeat the process.\n",
    "\n",
    "There's a tradeoff between exploration (choosing a random action) and exploitation (choosing actions based on already learned Q-values):\n",
    "- an additional parameter epsilon: the probability of exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 30000\n",
      "Training finished.\n",
      "\n",
      "Wall time: 47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1    # learning rate\n",
    "gamma = 0.6    # discount factor of the future rewards\n",
    "epsilon = 0.1  # probability of exploration vs exploitation\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 30001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties = 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "        \n",
    "    all_epochs.append(epochs)\n",
    "    all_penalties.append(penalties)\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x25a03308880>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# number of timesteps to finish the episode\n",
    "sns.lineplot(range(len(all_epochs)), all_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x25a03a92610>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# penalties incurred in each episode\n",
    "sns.lineplot(range(len(all_penalties)), all_penalties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [ -2.41777658,  -2.36322746,  -2.41801578,  -2.3635322 ,\n",
       "         -2.27325184, -11.32811778],\n",
       "       [ -1.86607845,  -1.45417117,  -1.85664844,  -1.45105493,\n",
       "         -0.7504    , -10.36207989],\n",
       "       ...,\n",
       "       [ -1.08831672,   0.36920534,  -1.08831672,  -1.09140694,\n",
       "         -2.7753397 ,  -1.95809354],\n",
       "       [ -2.13031397,  -2.11795723,  -2.1321916 ,  -2.11804834,\n",
       "         -3.71285004,  -4.37948439],\n",
       "       [  1.42144954,  -0.29302   ,   0.9051096 ,  10.99999848,\n",
       "         -1.65040201,  -2.04804828]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 13.18\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1            \n",
    "            \n",
    "        epochs += 1\n",
    "        \n",
    "        if ep < 100:\n",
    "            frames.append({\n",
    "                'frame': env.render(mode='ansi'),\n",
    "                'state': state,\n",
    "                'action': action,\n",
    "                'reward': reward\n",
    "                })\n",
    "\n",
    "        # clear_output(wait=True)\n",
    "        # print(f\"Episode: {ep}, timestep: {epochs}\")            \n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "\n",
      "Timestep: 1000\n",
      "State: 318\n",
      "Action: 0\n",
      "Reward: -1\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(frames[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
